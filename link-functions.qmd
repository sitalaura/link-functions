---
title: "Are we getting interactions wrong? <br> The role of link functions <br> in psychological research"
authors: "Laura Sità, Margherita Calderan, Tommaso Feraco, <br> Filippo Gambarota, Enrico Toffalini"
subtitle: |
  <br>
  <div style="display:flex; justify-content:center; align-items:center;">
    <img src="images/logo.png" style="height:155px; margin-right:3cm;">
    <img src="images/logo-UNIPD.bmp" style="height:150px;">
  </div>
format:
  revealjs:
    theme: custom.scss
    center: true
    embed-resources: true
    slide-number: true
    footer: "Cognitive Science Arena 2026"
    output-file: "link-functions.html"
    controls: false
toc: false
execute:
  cache: false
  freeze: false
---

```{r source}
#| echo: false
#| include: false
#source("R/datasim.R")

library(ggplot2)
library(effects)
library(psyphy)

ts <- 16
tc <- "black"
```

# 1 Example

## Simulated dataset 1

- Independent variable: **<span style="color:#800000;">age</span>** (in years)
- Dependent variable: **<span style="color:#800000;">IQ score</span>**

```{r sim1}
#| echo: false
set.seed(2)

N <- 200
age <- runif(N, 6, 10)
r <- 0.7
age_z <- scale(age)
iq_score <- r * age_z + sqrt(1 - r^2) * rnorm(N)
d <- data.frame(age, iq_score)

ggplot(d,aes(x=age,y=iq_score))+
  geom_point()+
  ylab("IQ score")
```

## Linear model

Using the classical linear predictor

```{r ex1fitL}
#| echo: true
#| eval: false
fit = lm(iq_score~age, data=d)
```

::: fragment
```{r ex1fitL-plot}
#| echo: false
eff <- data.frame(age = seq(min(age), max(age), length.out = 100))

fit <- lm(iq_score ~ age, data = d)
eff$y <- predict(fit, newdata = eff)

ggplot(d, aes(x = age, y = iq_score)) +
  geom_point() +
  geom_line(data = eff, aes(x = age, y = y),
            color = "red", linewidth = 2) +
  ylab("IQ score")
```
:::

## Linear model

What we don't see:

```{r fitL-link}
#| echo: true
#| eval: false
#| code-fold: true
fit = glm(iq_score~age, family=gaussian(link="identity"), data=d)
```

::: fragment
The model uses a Gaussian family with an identity link
:::

::: fragment
The **link function** maps the linear predictor
$\eta = \beta_0 + \beta_1 \cdot \text{age}$
onto the scale of the response variable $Y$
:::


# 2 Example

## Simulated dataset 2

- Independent variable: **<span style="color:#800000;">age</span>** (in years)
- Dependent variable: **<span style="color:#800000;">accuracy</span>** in a TRUE/FALSE task

```{r sim2}
#| echo: false

set.seed(0)

k = 60
N = 400
age = runif(N,6,10)
eta = -5.5+age*0.9
probs = mafc.probit(.m = 2)$linkinv(eta)
accuracy = rbinom(n = N, size = k, prob = probs) / k
d = data.frame(age,accuracy)

ggplot(d,aes(x=age,y=accuracy))+
  geom_point()
```

## Linear model

```{r fitL-plot}
#| echo: false
fit = lm(accuracy ~ age, data=d)
eff = data.frame(allEffects(fit,xlevels=list(age=seq(6,10,.1)))$"age")
eff$accuracy = eff$fit
ggplot(d,aes(x=age,y=accuracy))+
  geom_point()+
  geom_line(data=eff,color="red",size=2)
```

## Linear model

Using the classical linear predictor

```{r fitL}
#| echo: true
#| eval: false
fit = lm(accuracy~age, data=d)
```

::: notes
questo modello ci aiuta a predire i dati? per rispondere alla domanda usiamo questo trick: usiamo il modello costruito per creare un nuovo dataset. se il nuovo dataset somiglia ai dati che già possediamo, allora la risposta è "si, il modello ci aiuta a predire nuovi dati". se invece il dataset nuovo, che apparirà sotto forma di puntini viola (i nuovi dati) è diverso dal precedente, allora "no, il modello che ho costruito non predice bene nuovi dati, a partire da quelli di cui dispongo (simulati, come in questo caso, o osservati)"
:::

## Linear model

<span style="color:#800080;">New predicted values</span> fall outside the valid range for accuracy [0,1]

```{r fitP-plot2}
#| echo: false

d$pp_sim = simulate(fit)$sim_1

ggplot(d,aes(x=age,y=accuracy))+
  geom_point(aes(x=age,y=pp_sim),color="purple",size=3.5,alpha=.4)+
  geom_point()+
  geom_line(data=eff,color="red",size=2) +
  ylab("accuracy")
```

## ❌ Inappropriate model

In the first example, the identity link was appropriate  

- <span style="color:#800000;">IQ score</span> spans from $-\infty$ to $+\infty$

::: fragment
Here, the identity link is **not** appropriate  

- <span style="color:#800000;">accuracy</span> is bounded between 0 and 1
:::

::: notes
al pubblico: dobbiamo quindi cambiare modello, perché quello lineare non aiuta a predire nuovi dati CIOE non tiene conto dei bound dei data (0 e 1) domanda: quindi, che modello facciamo? risposta: regressione logistica
:::

## ✅ More appropriate model

```{r fitLogit-plot}
#| echo: false
#| warning: false

fit = glm(accuracy ~ age, data=d, family=binomial(link="logit"),
          weights= rep(k, nrow(d)))

eff = data.frame(allEffects(fit,xlevels=list(age=seq(6,10,.1)))$"age")
eff$accuracy = eff$fit
ggplot(d,aes(x=age,y=accuracy))+
  geom_point()+
  geom_line(data=eff,color="red",size=2)

```

::: notes
vediamo graficamente che il modello ci permette di predire nuovi outcome compresi tra 0 e 1 e se facciamo lo stesso trick vediamo che il nuovo dataset, viola, presenta valori che stanno nel range di valori possibili per la nostra vd
:::

## ✅ More appropriate model

<span style="color:#800080;">New predicted values</span> fall within the valid range for accuracy [0,1]

```{r sup3}
#| echo: false

eff = data.frame(allEffects(fit,xlevels=list(age=seq(6,10,.1)))$"age")
eff$accuracy = eff$fit

d$pp_sim = simulate(fit)$sim_1
ggplot(d,aes(x=age,y=accuracy))+
  geom_point(aes(x=age,y=pp_sim),color="purple",size=3.5,alpha=.4)+
  geom_point()+
  geom_line(data=eff,color="red",size=2) +
  ylab("accuracy")
```

## ✅ More appropriate model

```{r fitLogit}
#| echo: true
#| warning: false
fit = glm(accuracy ~ age, data=d, family=binomial(link="logit"), weights= rep(k, nrow(d)))
```

`link="logit"` makes sure that `y` spans from 0 and 1

# 3 Studying interactions

## Simulated dataset 2

- Independent variable: **<span style="color:#800000;">age</span>** (in years)
- Dependent variable: **<span style="color:#800000;">accuracy</span>** in a TRUE/FALSE task

::: fragment
**Adding a new main effect: <span style="color:#800000;">group</span>**

- Typically developing children (`group = 0`)
- Children with dyslexia (`group = 1`)
:::

## Simulated dataset 2

```{r sim2int}
#| echo: false

set.seed(0)

k = 50
N = 1000
group = rbinom(N,1,.5)
age = runif(N,6,10)
eta = -6+1*age-1*group
probs = mafc.probit(.m = 2)$linkinv(eta)
accuracy = rbinom(n = N, size = k, prob = probs) / k

d = data.frame(
  age = age,
  age_c = age - mean(age),
  accuracy = accuracy,
  group = as.factor(group)
)

ggplot(d, aes(x = age, y = accuracy, color = group)) +
  geom_point() +
  scale_x_continuous(limits = c(6, 10), breaks = seq(6, 10, 1))
```

## `link="identity"`

```{r fitLint-plot}
#| echo: false
#| warning: false

fitLint <- glm(accuracy ~ age * group, data = d)

eff <- data.frame(
  allEffects(
    fitLint,
    xlevels = list(age = seq(min(d$age), max(d$age), .05))
  )[["age:group"]]
)

ggplot(d, aes(x = age, y = accuracy, color = group)) +
  geom_point(alpha = .6) +
  geom_line(data = eff, aes(x = age, y = fit, group = group), size = 2)

```

## `link="identity"`

A **positive** interaction emerges

```{r fitLint}
#| echo: true
#| include: true
#| warning: true
fit = glm(accuracy ~ age*group, data=d)
summary(fit)
```

## `link="logit"`

```{r fitLogitint-plot}
#| echo: false
#| warning: false

fitLogitint = glm(accuracy ~ age*group, data=d, family=binomial(link="logit"),
          weights= rep(k, nrow(d)))

eff <- data.frame(
  allEffects(
    fitLogitint,
    xlevels = list(age = seq(min(d$age), max(d$age), .05))
  )[["age:group"]]
)

ggplot(d, aes(x = age, y = accuracy, color = group)) +
  geom_point(alpha = .6) +
  geom_line(
    data = eff,
    aes(x = age, y = fit, group = group),
    size = 2
  )
```

## `link="logit"`

A **negative** interaction emerges

```{r fitLogitint}
#| echo: true
#| include: true
#| warning: false

fit = glm(accuracy ~ age*group, data=d, family=binomial(link="logit"), weights= rep(k, nrow(d)))
summary(fit)
```

::: notes
domanda (per alzata di mano): quale dei due modelli riflette ciò che ho simulato? o nessuno dei due? riformula la domanda: in termini dell'esempio tipo all'aumentare dell'età,... e perché?
:::

## What was actually simulated

```{r datasim}
#| echo: true
#| code-fold: true

k = 50
N = 1000
group = rbinom(N,1,.5)
age = runif(N,6,10)
eta = -6+1*age-1*group
probs = mafc.probit(.m = 2)$linkinv(eta)
accuracy = rbinom(n = N, size = k, prob = probs) / k

d = data.frame(
  age = age,
  age_c = age - mean(age),
  accuracy = accuracy,
  group = as.factor(group)
)

ggplot(d, aes(x = age, y = accuracy, color = group)) +
  geom_point() +
  scale_x_continuous(limits = c(6, 10), breaks = seq(6, 10, 1))
```

::: fragment
No interaction was simulated

<span style="color:#800000;"> Both models are detecting **an interaction that does not exist** </span>
:::

::: notes
sappiamo perché il modello lineare non funziona (da prima). ma perché non anche il modello di regressione logistica? risposta: perché se tiro TUTTO a caso in un compito vero/falso di cui non si sa nulla del contenuto, allora l'accuratezza è del 50% non 0%! la regrssione logistica invece dice proprio questo: accuratezza dello 0% pertanto, anche se la family è giusta (perché ci permette di predire sempre valori che spannano tra 0 e 1), dobbiamo usare un link diverso, che tenga conto di sta cosa. il link in questione si chiama e ve lo mostro nel modello
:::

## ✅ The actual appropriate model

Using **2 alternatives forced-choice probit link**, 

we account for the 50\% chance level in a TRUE/FALSE task.

```{r fitMint-plot}
#| echo: false
#| warning: false

fitM = glm(accuracy ~ age*group, data=d, family=binomial(link=mafc.probit(.m=2)), weights= rep(k, nrow(d)))

eff <- data.frame(
  allEffects(
    fitM,
    xlevels = list(age = seq(min(d$age), max(d$age), .05))
  )[["age:group"]]
)

ggplot(d, aes(x = age, y = accuracy, color = group)) +
  geom_point(alpha = .6) +
  geom_line(
    data = eff,
    aes(x = age, y = fit, group = group),
    size = 2
  )
```

## ✅ The actual appropriate model: `link="mafc.probit(2)"`

No interaction emerges, in line with how the data were generated

```{r fitMint}
#| echo: true
#| warning: true

fit = glm(accuracy ~ age*group, data=d, family=binomial(link=mafc.probit(.m=2)), weights= rep(k, nrow(d)))
summary(fit)
```

# 4 Why interactions

## `link="identity"`

Equal intervals on X correspond to equal intervals on Y

```{r idlink}
#| echo: false
#| fig.align: "center" 

df_line <- data.frame(
  x = seq(-4.5, 4.5, length.out = 200)
)
df_line$y <- df_line$x


x_vals <- -4:4
y_vals <- -4:4

ggplot(df_line, aes(x = x, y = y)) +
  geom_hline(
    yintercept = y_vals,
    color = "grey60",
    linewidth = 0.6
  ) +
  geom_vline(
    xintercept = x_vals,
    color = "red",
    linetype = "dashed",
    linewidth = 0.6
  ) +
  geom_line(
    color = "blue",
    linewidth = 2
  ) +
  scale_x_continuous(
    limits = c(-4.5, 4.5),
    breaks = x_vals
  ) +
  scale_y_continuous(
    limits = c(-4.5, 4.5),
    breaks = y_vals
  ) +

  labs(
    x = "Linear predictor",
    y = "IQ score"
  ) +
  theme_minimal(base_size = 18) +
  theme(
    plot.title = element_text(face = "bold", color = "steelblue", size = 26),
    plot.subtitle = element_text(size = 18),
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 14)
  )
```

In our example: $\eta = \beta_0 + \beta_1 \cdot age$

## `link="logit"`

Equal intervals on X correspond to equal ratios (NOT equal intervals) on Y

```{r loglink}
#| echo: false
#| warning: false
#| fig.align: "center" 

df_line <- data.frame(
  x = seq(-5, 5, length.out = 400)
)
df_line$y <- plogis(df_line$x)

x_vals <- -4:4
y_vals <- plogis(x_vals)

ggplot(df_line, aes(x = x, y = y)) +

  geom_hline(
    yintercept = y_vals,
    color = "grey60",
    linewidth = 0.6
  ) +
  geom_vline(
    xintercept = x_vals,
    color = "red",
    linetype = "dashed",
    linewidth = 0.6
  ) +
  geom_line(
    color = "blue",
    linewidth = 2
  ) +
  
  scale_x_continuous(
    limits = c(-4.5, 4.5),
    breaks = x_vals
  ) +
  
  scale_y_continuous(
    limits = c(0, 1),
    breaks = round(y_vals, 2),
    labels = round(y_vals, 2)
  ) +
  
  labs(
    x = "Linear predictor",
    y = "Accuracy"
  ) +
  
  theme_minimal(base_size = 18) +
  theme(
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 14)
  )

```

## `link=mafc.probit(2)`

Equal intervals on X do NOT correspond to equal intervals on Y

```{r macflink}
#| echo: false
#| warning: false
#| fig.align: "center" 

mafc_probit_linkinv <- function(x, m) {
  g <- 1 / m
  g + (1 - g) * pnorm(x)
}

df_line <- data.frame(x = seq(-4.5, 4.5, length.out = 400))
df_line$y <- mafc_probit_linkinv(df_line$x, m = 2)

x_vals <- -4:4
y_vals <- mafc_probit_linkinv(x_vals, m = 2)

y_breaks <- mafc_probit_linkinv(c(-4, -1, 0, 1, 2, 3, 4), m = 2)
y_breaks <- round(y_breaks, 2)

y_breaks <- y_breaks[y_breaks < 1]
y_breaks <- c(y_breaks, 0.99)

ggplot(df_line, aes(x = x, y = y)) +
  geom_hline(yintercept = y_vals, color = "grey60", linewidth = 0.6) +
  geom_vline(xintercept = x_vals, color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_line(color = "blue", linewidth = 2) +
  geom_hline(yintercept = 1/2, linewidth = 1) +
  scale_x_continuous(limits = c(-4.5, 4.5), breaks = x_vals) +
  scale_y_continuous(
    limits = c(0.25, 1),
    breaks = y_breaks,
    labels = y_breaks
  ) +
  labs(x = "Linear predictor", y = "Accuracy") +
  theme_minimal(base_size = 18) +
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 14))


```

# Conclusions

##

Building a model means approximating the data-generating process (never observed directly in real data)

That requires key choices:

::: {.callout-tip}
**Choose an appropriate distribution (family):**
<br> predicted values remain within the outcome’s valid range
:::

::: {.callout-tip}
**Choose an appropriate link function: **  
the link defines the scale of effects
<br> the wrong link can create spurious interactions
:::

## Our work

We are conducting a [systematic review](https://osf.io/tpjax/overview?view_only=7feccf54ff484fe093cf65579617b215) to assess how often inappropriate link functions are used when testing interactions in psychological research

::: fragment
So far, this appears to happen quite often
:::

## Materials & Contact

Data simulation, code and presentation are available on GitHub: <a href="https://github.com/sitalaura/link-functions.git" target="_blank">sitalaura/link-functions</a>

Questions and feedbacks: <a href="mailto: laura.sita@studenti.unipd.it" target="_blank">laura.sita\@studenti.unipd.it</a>

\vspace{1cm}

::::: columns
::: {.column width="45%"}
![](images/logo.png){fig-align="center" width="180"}
:::

::: {.column width="55%"}
![](images/logo-UNIPD.bmp){fig-align="center" width="450"}
:::
:::::

## Bibliography

<p style="font-size:70%; text-align:left; line-height:1.2; padding-left:1.5em; text-indent:-1.5em;">
Domingue, B. W., Kanopka, K., Trejo, S., Rhemtulla, M., & Tucker-Drob, E. M. (2024). Ubiquitous bias and false discovery due to model misspecification in analysis of statistical interactions: The role of the outcome’s distribution and metric properties. *Psychological methods, 29*(6), 1164.
</p>

<p style="font-size:70%; text-align:left; line-height:1.2; padding-left:1.5em; text-indent:-1.5em;">
Hardwicke, T. E., Thibault, R. T., Clarke, B., Moodie, N., Crüwell, S., Schiavone, S. R., Handcock, S. A., Nghiem, K. A., Mody, F., Eerola, T., et al. (2024). Prevalence of transparent research practices in psychology: A cross-sectional study of empirical articles published in 2022. *Advances in Methods and Practices in Psychological Science, 7* (4), 25152459241283477.
</p>

<p style="font-size:70%; text-align:left; line-height:1.2; padding-left:1.5em; text-indent:-1.5em;">
Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong?. *Journal of Experimental Social Psychology, 79*, 328-348.
</p>

<p style="font-size:70%; text-align:left; line-height:1.2; padding-left:1.5em; text-indent:-1.5em;">
Micceri, T. (1989). The unicorn, the normal curve, and other improbable creatures. *Psychological bulletin, 105*(1), 156.
</p>

## Thank you

Special thanks to

# Supplementary materials

```{r sim2sup}
#| echo: false
set.seed(0)

k = 60
N = 400
age = runif(N,6,10)
eta = -5.5+age*0.9
probs = mafc.probit(.m = 2)$linkinv(eta)
accuracy = rbinom(n = N, size = k, prob = probs) / k
d = data.frame(age,accuracy)
```

## New predicted values with `link="probit"`

```{r sup1}
#| echo: true
#| warning: false
fit = glm(accuracy ~ age, data=d, family=binomial(link="probit"), weights= rep(k, nrow(d)))
```

```{r sup2}
#| echo: false

eff = data.frame(allEffects(fit,xlevels=list(age=seq(6,10,.1)))$"age")
eff$accuracy = eff$fit

d$pp_sim = simulate(fit)$sim_1
ggplot(d,aes(x=age,y=accuracy))+
  geom_point(aes(x=age,y=pp_sim),color="purple",size=3.5,alpha=.4)+
  geom_point()+
  geom_line(data=eff,color="red",size=2) +
  ylab("accuracy")
```

## `link="probit"`

A **negative** interaction emerges

```{r sim2intsup}
#| echo: false
set.seed(0)

k = 50
N = 1000
group = rbinom(N,1,.5)
age = runif(N,6,10)
eta = -6+1*age-1*group
probs = mafc.probit(.m = 2)$linkinv(eta)
accuracy = rbinom(n = N, size = k, prob = probs) / k
d = data.frame(age=age-mean(age),accuracy,group=as.factor(group))
```

```{r sup4}
#| echo: true
fit = glm(accuracy ~ age*group, data=d, family=binomial(link="probit"), weights= rep(k, nrow(d)))
summary(fit)
```
